{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishaanpaul98/Sentiment-Analysis/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pGQVQfr0b3SS"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen, Request\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "import datetime\n",
        "import yfinance as yf\n",
        "import time\n",
        "import requests\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WSgZEvuikgTn"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9hbdUWDKkgv_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Eqvj106lkgyU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,classification_report\n",
        "from sklearn.metrics import plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stock Data Helper Functions #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting stock data for stock $AAPL\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2022-12-21</th>\n",
              "      <td>132.979996</td>\n",
              "      <td>136.809998</td>\n",
              "      <td>132.750000</td>\n",
              "      <td>135.449997</td>\n",
              "      <td>135.449997</td>\n",
              "      <td>85928000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-22</th>\n",
              "      <td>134.350006</td>\n",
              "      <td>134.559998</td>\n",
              "      <td>130.300003</td>\n",
              "      <td>132.229996</td>\n",
              "      <td>132.229996</td>\n",
              "      <td>77852100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-23</th>\n",
              "      <td>130.919998</td>\n",
              "      <td>132.419998</td>\n",
              "      <td>129.639999</td>\n",
              "      <td>131.860001</td>\n",
              "      <td>131.860001</td>\n",
              "      <td>63814900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-27</th>\n",
              "      <td>131.380005</td>\n",
              "      <td>131.410004</td>\n",
              "      <td>128.720001</td>\n",
              "      <td>130.029999</td>\n",
              "      <td>130.029999</td>\n",
              "      <td>69007800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-28</th>\n",
              "      <td>129.669998</td>\n",
              "      <td>131.029999</td>\n",
              "      <td>125.870003</td>\n",
              "      <td>126.040001</td>\n",
              "      <td>126.040001</td>\n",
              "      <td>85438400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-29</th>\n",
              "      <td>127.989998</td>\n",
              "      <td>130.479996</td>\n",
              "      <td>127.730003</td>\n",
              "      <td>129.610001</td>\n",
              "      <td>129.610001</td>\n",
              "      <td>75703700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-30</th>\n",
              "      <td>128.410004</td>\n",
              "      <td>129.949997</td>\n",
              "      <td>127.430000</td>\n",
              "      <td>129.929993</td>\n",
              "      <td>129.929993</td>\n",
              "      <td>76960600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-03</th>\n",
              "      <td>130.279999</td>\n",
              "      <td>130.899994</td>\n",
              "      <td>124.169998</td>\n",
              "      <td>125.070000</td>\n",
              "      <td>125.070000</td>\n",
              "      <td>112117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-04</th>\n",
              "      <td>126.889999</td>\n",
              "      <td>128.660004</td>\n",
              "      <td>125.080002</td>\n",
              "      <td>126.360001</td>\n",
              "      <td>126.360001</td>\n",
              "      <td>89113600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-05</th>\n",
              "      <td>127.129997</td>\n",
              "      <td>127.769997</td>\n",
              "      <td>124.760002</td>\n",
              "      <td>125.019997</td>\n",
              "      <td>125.019997</td>\n",
              "      <td>80962700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-06</th>\n",
              "      <td>126.010002</td>\n",
              "      <td>130.289993</td>\n",
              "      <td>124.889999</td>\n",
              "      <td>129.619995</td>\n",
              "      <td>129.619995</td>\n",
              "      <td>87686600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-09</th>\n",
              "      <td>130.470001</td>\n",
              "      <td>133.410004</td>\n",
              "      <td>129.889999</td>\n",
              "      <td>130.149994</td>\n",
              "      <td>130.149994</td>\n",
              "      <td>70790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-10</th>\n",
              "      <td>130.259995</td>\n",
              "      <td>131.259995</td>\n",
              "      <td>128.119995</td>\n",
              "      <td>130.729996</td>\n",
              "      <td>130.729996</td>\n",
              "      <td>63896200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-11</th>\n",
              "      <td>131.250000</td>\n",
              "      <td>133.509995</td>\n",
              "      <td>130.460007</td>\n",
              "      <td>133.490005</td>\n",
              "      <td>133.490005</td>\n",
              "      <td>69458900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-12</th>\n",
              "      <td>133.880005</td>\n",
              "      <td>134.259995</td>\n",
              "      <td>131.440002</td>\n",
              "      <td>133.410004</td>\n",
              "      <td>133.410004</td>\n",
              "      <td>71379600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-13</th>\n",
              "      <td>132.029999</td>\n",
              "      <td>134.919998</td>\n",
              "      <td>131.660004</td>\n",
              "      <td>134.759995</td>\n",
              "      <td>134.759995</td>\n",
              "      <td>57758000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-17</th>\n",
              "      <td>134.830002</td>\n",
              "      <td>137.289993</td>\n",
              "      <td>134.130005</td>\n",
              "      <td>135.940002</td>\n",
              "      <td>135.940002</td>\n",
              "      <td>63646600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-18</th>\n",
              "      <td>136.820007</td>\n",
              "      <td>138.610001</td>\n",
              "      <td>135.029999</td>\n",
              "      <td>135.210007</td>\n",
              "      <td>135.210007</td>\n",
              "      <td>69672800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-19</th>\n",
              "      <td>134.080002</td>\n",
              "      <td>136.250000</td>\n",
              "      <td>133.770004</td>\n",
              "      <td>135.270004</td>\n",
              "      <td>135.270004</td>\n",
              "      <td>58280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-20</th>\n",
              "      <td>135.279999</td>\n",
              "      <td>138.020004</td>\n",
              "      <td>134.220001</td>\n",
              "      <td>137.869995</td>\n",
              "      <td>137.869995</td>\n",
              "      <td>79972200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-23</th>\n",
              "      <td>138.119995</td>\n",
              "      <td>143.320007</td>\n",
              "      <td>137.899994</td>\n",
              "      <td>141.110001</td>\n",
              "      <td>141.110001</td>\n",
              "      <td>81760300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-24</th>\n",
              "      <td>140.309998</td>\n",
              "      <td>143.160004</td>\n",
              "      <td>140.300003</td>\n",
              "      <td>142.529999</td>\n",
              "      <td>142.529999</td>\n",
              "      <td>66435100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-25</th>\n",
              "      <td>140.889999</td>\n",
              "      <td>142.429993</td>\n",
              "      <td>138.809998</td>\n",
              "      <td>141.860001</td>\n",
              "      <td>141.860001</td>\n",
              "      <td>65799300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-26</th>\n",
              "      <td>143.169998</td>\n",
              "      <td>144.250000</td>\n",
              "      <td>141.899994</td>\n",
              "      <td>143.960007</td>\n",
              "      <td>143.960007</td>\n",
              "      <td>54105100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-27</th>\n",
              "      <td>143.160004</td>\n",
              "      <td>147.229996</td>\n",
              "      <td>143.080002</td>\n",
              "      <td>145.929993</td>\n",
              "      <td>145.929993</td>\n",
              "      <td>70492800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-30</th>\n",
              "      <td>144.960007</td>\n",
              "      <td>145.550003</td>\n",
              "      <td>142.850006</td>\n",
              "      <td>143.000000</td>\n",
              "      <td>143.000000</td>\n",
              "      <td>64015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-31</th>\n",
              "      <td>142.699997</td>\n",
              "      <td>144.339996</td>\n",
              "      <td>142.279999</td>\n",
              "      <td>144.289993</td>\n",
              "      <td>144.289993</td>\n",
              "      <td>65874500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-02-01</th>\n",
              "      <td>143.970001</td>\n",
              "      <td>146.610001</td>\n",
              "      <td>141.320007</td>\n",
              "      <td>145.429993</td>\n",
              "      <td>145.429993</td>\n",
              "      <td>77663600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-02-02</th>\n",
              "      <td>148.899994</td>\n",
              "      <td>151.179993</td>\n",
              "      <td>148.169998</td>\n",
              "      <td>150.820007</td>\n",
              "      <td>150.820007</td>\n",
              "      <td>118339000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-02-03</th>\n",
              "      <td>148.029999</td>\n",
              "      <td>157.380005</td>\n",
              "      <td>147.830002</td>\n",
              "      <td>154.500000</td>\n",
              "      <td>154.500000</td>\n",
              "      <td>149918017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Open        High         Low       Close   Adj Close  \\\n",
              "Date                                                                     \n",
              "2022-12-21  132.979996  136.809998  132.750000  135.449997  135.449997   \n",
              "2022-12-22  134.350006  134.559998  130.300003  132.229996  132.229996   \n",
              "2022-12-23  130.919998  132.419998  129.639999  131.860001  131.860001   \n",
              "2022-12-27  131.380005  131.410004  128.720001  130.029999  130.029999   \n",
              "2022-12-28  129.669998  131.029999  125.870003  126.040001  126.040001   \n",
              "2022-12-29  127.989998  130.479996  127.730003  129.610001  129.610001   \n",
              "2022-12-30  128.410004  129.949997  127.430000  129.929993  129.929993   \n",
              "2023-01-03  130.279999  130.899994  124.169998  125.070000  125.070000   \n",
              "2023-01-04  126.889999  128.660004  125.080002  126.360001  126.360001   \n",
              "2023-01-05  127.129997  127.769997  124.760002  125.019997  125.019997   \n",
              "2023-01-06  126.010002  130.289993  124.889999  129.619995  129.619995   \n",
              "2023-01-09  130.470001  133.410004  129.889999  130.149994  130.149994   \n",
              "2023-01-10  130.259995  131.259995  128.119995  130.729996  130.729996   \n",
              "2023-01-11  131.250000  133.509995  130.460007  133.490005  133.490005   \n",
              "2023-01-12  133.880005  134.259995  131.440002  133.410004  133.410004   \n",
              "2023-01-13  132.029999  134.919998  131.660004  134.759995  134.759995   \n",
              "2023-01-17  134.830002  137.289993  134.130005  135.940002  135.940002   \n",
              "2023-01-18  136.820007  138.610001  135.029999  135.210007  135.210007   \n",
              "2023-01-19  134.080002  136.250000  133.770004  135.270004  135.270004   \n",
              "2023-01-20  135.279999  138.020004  134.220001  137.869995  137.869995   \n",
              "2023-01-23  138.119995  143.320007  137.899994  141.110001  141.110001   \n",
              "2023-01-24  140.309998  143.160004  140.300003  142.529999  142.529999   \n",
              "2023-01-25  140.889999  142.429993  138.809998  141.860001  141.860001   \n",
              "2023-01-26  143.169998  144.250000  141.899994  143.960007  143.960007   \n",
              "2023-01-27  143.160004  147.229996  143.080002  145.929993  145.929993   \n",
              "2023-01-30  144.960007  145.550003  142.850006  143.000000  143.000000   \n",
              "2023-01-31  142.699997  144.339996  142.279999  144.289993  144.289993   \n",
              "2023-02-01  143.970001  146.610001  141.320007  145.429993  145.429993   \n",
              "2023-02-02  148.899994  151.179993  148.169998  150.820007  150.820007   \n",
              "2023-02-03  148.029999  157.380005  147.830002  154.500000  154.500000   \n",
              "\n",
              "               Volume  \n",
              "Date                   \n",
              "2022-12-21   85928000  \n",
              "2022-12-22   77852100  \n",
              "2022-12-23   63814900  \n",
              "2022-12-27   69007800  \n",
              "2022-12-28   85438400  \n",
              "2022-12-29   75703700  \n",
              "2022-12-30   76960600  \n",
              "2023-01-03  112117500  \n",
              "2023-01-04   89113600  \n",
              "2023-01-05   80962700  \n",
              "2023-01-06   87686600  \n",
              "2023-01-09   70790800  \n",
              "2023-01-10   63896200  \n",
              "2023-01-11   69458900  \n",
              "2023-01-12   71379600  \n",
              "2023-01-13   57758000  \n",
              "2023-01-17   63646600  \n",
              "2023-01-18   69672800  \n",
              "2023-01-19   58280400  \n",
              "2023-01-20   79972200  \n",
              "2023-01-23   81760300  \n",
              "2023-01-24   66435100  \n",
              "2023-01-25   65799300  \n",
              "2023-01-26   54105100  \n",
              "2023-01-27   70492800  \n",
              "2023-01-30   64015300  \n",
              "2023-01-31   65874500  \n",
              "2023-02-01   77663600  \n",
              "2023-02-02  118339000  \n",
              "2023-02-03  149918017  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def getStockDataDaily(symbol, day):\n",
        "    print(\"Getting stock data for stock $\"+symbol)\n",
        "    df = yf.download(symbol, start=day, period = \"1d\")\n",
        "    return df\n",
        "\n",
        "getStockDataDaily('AAPL', \"2022-12-21\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def arrayToString(arr):\n",
        "    print(\"Starting array to list\")\n",
        "    listToStr = ' '.join([str(elem) for elem in arr])\n",
        "    return listToStr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of symbols array is more than 1. STARTING ARRAYTOSTRING\n",
            "Starting array to list\n",
            "Getting stock data for stock $AAPL TSLA\n",
            "[*********************100%***********************]  2 of 2 completed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"6\" halign=\"left\">TSLA</th>\n",
              "      <th colspan=\"6\" halign=\"left\">AAPL</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2022-12-21</th>\n",
              "      <td>139.339996</td>\n",
              "      <td>141.259995</td>\n",
              "      <td>135.889999</td>\n",
              "      <td>137.570007</td>\n",
              "      <td>137.570007</td>\n",
              "      <td>145417400</td>\n",
              "      <td>132.979996</td>\n",
              "      <td>136.809998</td>\n",
              "      <td>132.750000</td>\n",
              "      <td>135.449997</td>\n",
              "      <td>135.449997</td>\n",
              "      <td>85928000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-22</th>\n",
              "      <td>136.000000</td>\n",
              "      <td>136.630005</td>\n",
              "      <td>122.260002</td>\n",
              "      <td>125.349998</td>\n",
              "      <td>125.349998</td>\n",
              "      <td>210090300</td>\n",
              "      <td>134.350006</td>\n",
              "      <td>134.559998</td>\n",
              "      <td>130.300003</td>\n",
              "      <td>132.229996</td>\n",
              "      <td>132.229996</td>\n",
              "      <td>77852100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-23</th>\n",
              "      <td>126.370003</td>\n",
              "      <td>128.619995</td>\n",
              "      <td>121.019997</td>\n",
              "      <td>123.150002</td>\n",
              "      <td>123.150002</td>\n",
              "      <td>166989700</td>\n",
              "      <td>130.919998</td>\n",
              "      <td>132.419998</td>\n",
              "      <td>129.639999</td>\n",
              "      <td>131.860001</td>\n",
              "      <td>131.860001</td>\n",
              "      <td>63814900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-27</th>\n",
              "      <td>117.500000</td>\n",
              "      <td>119.669998</td>\n",
              "      <td>108.760002</td>\n",
              "      <td>109.099998</td>\n",
              "      <td>109.099998</td>\n",
              "      <td>208643400</td>\n",
              "      <td>131.380005</td>\n",
              "      <td>131.410004</td>\n",
              "      <td>128.720001</td>\n",
              "      <td>130.029999</td>\n",
              "      <td>130.029999</td>\n",
              "      <td>69007800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-28</th>\n",
              "      <td>110.349998</td>\n",
              "      <td>116.269997</td>\n",
              "      <td>108.239998</td>\n",
              "      <td>112.709999</td>\n",
              "      <td>112.709999</td>\n",
              "      <td>221070500</td>\n",
              "      <td>129.669998</td>\n",
              "      <td>131.029999</td>\n",
              "      <td>125.870003</td>\n",
              "      <td>126.040001</td>\n",
              "      <td>126.040001</td>\n",
              "      <td>85438400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-29</th>\n",
              "      <td>120.389999</td>\n",
              "      <td>123.570000</td>\n",
              "      <td>117.500000</td>\n",
              "      <td>121.820000</td>\n",
              "      <td>121.820000</td>\n",
              "      <td>221923300</td>\n",
              "      <td>127.989998</td>\n",
              "      <td>130.479996</td>\n",
              "      <td>127.730003</td>\n",
              "      <td>129.610001</td>\n",
              "      <td>129.610001</td>\n",
              "      <td>75703700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-30</th>\n",
              "      <td>119.949997</td>\n",
              "      <td>124.480003</td>\n",
              "      <td>119.750000</td>\n",
              "      <td>123.180000</td>\n",
              "      <td>123.180000</td>\n",
              "      <td>157304500</td>\n",
              "      <td>128.410004</td>\n",
              "      <td>129.949997</td>\n",
              "      <td>127.430000</td>\n",
              "      <td>129.929993</td>\n",
              "      <td>129.929993</td>\n",
              "      <td>76960600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-03</th>\n",
              "      <td>118.470001</td>\n",
              "      <td>118.800003</td>\n",
              "      <td>104.639999</td>\n",
              "      <td>108.099998</td>\n",
              "      <td>108.099998</td>\n",
              "      <td>231402800</td>\n",
              "      <td>130.279999</td>\n",
              "      <td>130.899994</td>\n",
              "      <td>124.169998</td>\n",
              "      <td>125.070000</td>\n",
              "      <td>125.070000</td>\n",
              "      <td>112117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-04</th>\n",
              "      <td>109.110001</td>\n",
              "      <td>114.589996</td>\n",
              "      <td>107.519997</td>\n",
              "      <td>113.639999</td>\n",
              "      <td>113.639999</td>\n",
              "      <td>180389000</td>\n",
              "      <td>126.889999</td>\n",
              "      <td>128.660004</td>\n",
              "      <td>125.080002</td>\n",
              "      <td>126.360001</td>\n",
              "      <td>126.360001</td>\n",
              "      <td>89113600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-05</th>\n",
              "      <td>110.510002</td>\n",
              "      <td>111.750000</td>\n",
              "      <td>107.160004</td>\n",
              "      <td>110.339996</td>\n",
              "      <td>110.339996</td>\n",
              "      <td>157986300</td>\n",
              "      <td>127.129997</td>\n",
              "      <td>127.769997</td>\n",
              "      <td>124.760002</td>\n",
              "      <td>125.019997</td>\n",
              "      <td>125.019997</td>\n",
              "      <td>80962700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-06</th>\n",
              "      <td>103.000000</td>\n",
              "      <td>114.389999</td>\n",
              "      <td>101.809998</td>\n",
              "      <td>113.059998</td>\n",
              "      <td>113.059998</td>\n",
              "      <td>220575900</td>\n",
              "      <td>126.010002</td>\n",
              "      <td>130.289993</td>\n",
              "      <td>124.889999</td>\n",
              "      <td>129.619995</td>\n",
              "      <td>129.619995</td>\n",
              "      <td>87686600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-09</th>\n",
              "      <td>118.959999</td>\n",
              "      <td>123.519997</td>\n",
              "      <td>117.110001</td>\n",
              "      <td>119.769997</td>\n",
              "      <td>119.769997</td>\n",
              "      <td>190284000</td>\n",
              "      <td>130.470001</td>\n",
              "      <td>133.410004</td>\n",
              "      <td>129.889999</td>\n",
              "      <td>130.149994</td>\n",
              "      <td>130.149994</td>\n",
              "      <td>70790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-10</th>\n",
              "      <td>121.070000</td>\n",
              "      <td>122.760002</td>\n",
              "      <td>114.919998</td>\n",
              "      <td>118.849998</td>\n",
              "      <td>118.849998</td>\n",
              "      <td>167642500</td>\n",
              "      <td>130.259995</td>\n",
              "      <td>131.259995</td>\n",
              "      <td>128.119995</td>\n",
              "      <td>130.729996</td>\n",
              "      <td>130.729996</td>\n",
              "      <td>63896200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-11</th>\n",
              "      <td>122.089996</td>\n",
              "      <td>125.949997</td>\n",
              "      <td>120.510002</td>\n",
              "      <td>123.220001</td>\n",
              "      <td>123.220001</td>\n",
              "      <td>183810800</td>\n",
              "      <td>131.250000</td>\n",
              "      <td>133.509995</td>\n",
              "      <td>130.460007</td>\n",
              "      <td>133.490005</td>\n",
              "      <td>133.490005</td>\n",
              "      <td>69458900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-12</th>\n",
              "      <td>122.559998</td>\n",
              "      <td>124.129997</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>123.559998</td>\n",
              "      <td>123.559998</td>\n",
              "      <td>169400900</td>\n",
              "      <td>133.880005</td>\n",
              "      <td>134.259995</td>\n",
              "      <td>131.440002</td>\n",
              "      <td>133.410004</td>\n",
              "      <td>133.410004</td>\n",
              "      <td>71379600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-13</th>\n",
              "      <td>116.550003</td>\n",
              "      <td>122.629997</td>\n",
              "      <td>115.599998</td>\n",
              "      <td>122.400002</td>\n",
              "      <td>122.400002</td>\n",
              "      <td>180439300</td>\n",
              "      <td>132.029999</td>\n",
              "      <td>134.919998</td>\n",
              "      <td>131.660004</td>\n",
              "      <td>134.759995</td>\n",
              "      <td>134.759995</td>\n",
              "      <td>57758000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-17</th>\n",
              "      <td>125.699997</td>\n",
              "      <td>131.699997</td>\n",
              "      <td>125.019997</td>\n",
              "      <td>131.490005</td>\n",
              "      <td>131.490005</td>\n",
              "      <td>186477000</td>\n",
              "      <td>134.830002</td>\n",
              "      <td>137.289993</td>\n",
              "      <td>134.130005</td>\n",
              "      <td>135.940002</td>\n",
              "      <td>135.940002</td>\n",
              "      <td>63646600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-18</th>\n",
              "      <td>136.559998</td>\n",
              "      <td>136.679993</td>\n",
              "      <td>127.010002</td>\n",
              "      <td>128.779999</td>\n",
              "      <td>128.779999</td>\n",
              "      <td>195680300</td>\n",
              "      <td>136.820007</td>\n",
              "      <td>138.610001</td>\n",
              "      <td>135.029999</td>\n",
              "      <td>135.210007</td>\n",
              "      <td>135.210007</td>\n",
              "      <td>69672800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-19</th>\n",
              "      <td>127.260002</td>\n",
              "      <td>129.990005</td>\n",
              "      <td>124.309998</td>\n",
              "      <td>127.169998</td>\n",
              "      <td>127.169998</td>\n",
              "      <td>170291900</td>\n",
              "      <td>134.080002</td>\n",
              "      <td>136.250000</td>\n",
              "      <td>133.770004</td>\n",
              "      <td>135.270004</td>\n",
              "      <td>135.270004</td>\n",
              "      <td>58280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-20</th>\n",
              "      <td>128.679993</td>\n",
              "      <td>133.509995</td>\n",
              "      <td>127.349998</td>\n",
              "      <td>133.419998</td>\n",
              "      <td>133.419998</td>\n",
              "      <td>138429900</td>\n",
              "      <td>135.279999</td>\n",
              "      <td>138.020004</td>\n",
              "      <td>134.220001</td>\n",
              "      <td>137.869995</td>\n",
              "      <td>137.869995</td>\n",
              "      <td>79972200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-23</th>\n",
              "      <td>135.869995</td>\n",
              "      <td>145.380005</td>\n",
              "      <td>134.270004</td>\n",
              "      <td>143.750000</td>\n",
              "      <td>143.750000</td>\n",
              "      <td>203119200</td>\n",
              "      <td>138.119995</td>\n",
              "      <td>143.320007</td>\n",
              "      <td>137.899994</td>\n",
              "      <td>141.110001</td>\n",
              "      <td>141.110001</td>\n",
              "      <td>81760300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-24</th>\n",
              "      <td>143.000000</td>\n",
              "      <td>146.500000</td>\n",
              "      <td>141.100006</td>\n",
              "      <td>143.889999</td>\n",
              "      <td>143.889999</td>\n",
              "      <td>158699100</td>\n",
              "      <td>140.309998</td>\n",
              "      <td>143.160004</td>\n",
              "      <td>140.300003</td>\n",
              "      <td>142.529999</td>\n",
              "      <td>142.529999</td>\n",
              "      <td>66435100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-25</th>\n",
              "      <td>141.910004</td>\n",
              "      <td>146.410004</td>\n",
              "      <td>138.070007</td>\n",
              "      <td>144.429993</td>\n",
              "      <td>144.429993</td>\n",
              "      <td>192734300</td>\n",
              "      <td>140.889999</td>\n",
              "      <td>142.429993</td>\n",
              "      <td>138.809998</td>\n",
              "      <td>141.860001</td>\n",
              "      <td>141.860001</td>\n",
              "      <td>65799300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-26</th>\n",
              "      <td>159.970001</td>\n",
              "      <td>161.419998</td>\n",
              "      <td>154.759995</td>\n",
              "      <td>160.270004</td>\n",
              "      <td>160.270004</td>\n",
              "      <td>234815100</td>\n",
              "      <td>143.169998</td>\n",
              "      <td>144.250000</td>\n",
              "      <td>141.899994</td>\n",
              "      <td>143.960007</td>\n",
              "      <td>143.960007</td>\n",
              "      <td>54105100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-27</th>\n",
              "      <td>162.429993</td>\n",
              "      <td>180.679993</td>\n",
              "      <td>161.169998</td>\n",
              "      <td>177.899994</td>\n",
              "      <td>177.899994</td>\n",
              "      <td>305632100</td>\n",
              "      <td>143.160004</td>\n",
              "      <td>147.229996</td>\n",
              "      <td>143.080002</td>\n",
              "      <td>145.929993</td>\n",
              "      <td>145.929993</td>\n",
              "      <td>70492800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-30</th>\n",
              "      <td>178.050003</td>\n",
              "      <td>179.770004</td>\n",
              "      <td>166.500000</td>\n",
              "      <td>166.660004</td>\n",
              "      <td>166.660004</td>\n",
              "      <td>230878800</td>\n",
              "      <td>144.960007</td>\n",
              "      <td>145.550003</td>\n",
              "      <td>142.850006</td>\n",
              "      <td>143.000000</td>\n",
              "      <td>143.000000</td>\n",
              "      <td>64015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-31</th>\n",
              "      <td>164.570007</td>\n",
              "      <td>174.300003</td>\n",
              "      <td>162.779999</td>\n",
              "      <td>173.220001</td>\n",
              "      <td>173.220001</td>\n",
              "      <td>196813500</td>\n",
              "      <td>142.699997</td>\n",
              "      <td>144.339996</td>\n",
              "      <td>142.279999</td>\n",
              "      <td>144.289993</td>\n",
              "      <td>144.289993</td>\n",
              "      <td>65874500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-02-01</th>\n",
              "      <td>173.889999</td>\n",
              "      <td>183.809998</td>\n",
              "      <td>169.929993</td>\n",
              "      <td>181.410004</td>\n",
              "      <td>181.410004</td>\n",
              "      <td>213806300</td>\n",
              "      <td>143.970001</td>\n",
              "      <td>146.610001</td>\n",
              "      <td>141.320007</td>\n",
              "      <td>145.429993</td>\n",
              "      <td>145.429993</td>\n",
              "      <td>77663600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-02-02</th>\n",
              "      <td>187.330002</td>\n",
              "      <td>196.750000</td>\n",
              "      <td>182.610001</td>\n",
              "      <td>188.270004</td>\n",
              "      <td>188.270004</td>\n",
              "      <td>217448300</td>\n",
              "      <td>148.899994</td>\n",
              "      <td>151.179993</td>\n",
              "      <td>148.169998</td>\n",
              "      <td>150.820007</td>\n",
              "      <td>150.820007</td>\n",
              "      <td>118339000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-02-03</th>\n",
              "      <td>183.949997</td>\n",
              "      <td>198.990005</td>\n",
              "      <td>183.690002</td>\n",
              "      <td>189.979996</td>\n",
              "      <td>189.979996</td>\n",
              "      <td>229175706</td>\n",
              "      <td>148.029999</td>\n",
              "      <td>157.380005</td>\n",
              "      <td>147.830002</td>\n",
              "      <td>154.500000</td>\n",
              "      <td>154.500000</td>\n",
              "      <td>149918017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  TSLA                                                  \\\n",
              "                  Open        High         Low       Close   Adj Close   \n",
              "Date                                                                     \n",
              "2022-12-21  139.339996  141.259995  135.889999  137.570007  137.570007   \n",
              "2022-12-22  136.000000  136.630005  122.260002  125.349998  125.349998   \n",
              "2022-12-23  126.370003  128.619995  121.019997  123.150002  123.150002   \n",
              "2022-12-27  117.500000  119.669998  108.760002  109.099998  109.099998   \n",
              "2022-12-28  110.349998  116.269997  108.239998  112.709999  112.709999   \n",
              "2022-12-29  120.389999  123.570000  117.500000  121.820000  121.820000   \n",
              "2022-12-30  119.949997  124.480003  119.750000  123.180000  123.180000   \n",
              "2023-01-03  118.470001  118.800003  104.639999  108.099998  108.099998   \n",
              "2023-01-04  109.110001  114.589996  107.519997  113.639999  113.639999   \n",
              "2023-01-05  110.510002  111.750000  107.160004  110.339996  110.339996   \n",
              "2023-01-06  103.000000  114.389999  101.809998  113.059998  113.059998   \n",
              "2023-01-09  118.959999  123.519997  117.110001  119.769997  119.769997   \n",
              "2023-01-10  121.070000  122.760002  114.919998  118.849998  118.849998   \n",
              "2023-01-11  122.089996  125.949997  120.510002  123.220001  123.220001   \n",
              "2023-01-12  122.559998  124.129997  117.000000  123.559998  123.559998   \n",
              "2023-01-13  116.550003  122.629997  115.599998  122.400002  122.400002   \n",
              "2023-01-17  125.699997  131.699997  125.019997  131.490005  131.490005   \n",
              "2023-01-18  136.559998  136.679993  127.010002  128.779999  128.779999   \n",
              "2023-01-19  127.260002  129.990005  124.309998  127.169998  127.169998   \n",
              "2023-01-20  128.679993  133.509995  127.349998  133.419998  133.419998   \n",
              "2023-01-23  135.869995  145.380005  134.270004  143.750000  143.750000   \n",
              "2023-01-24  143.000000  146.500000  141.100006  143.889999  143.889999   \n",
              "2023-01-25  141.910004  146.410004  138.070007  144.429993  144.429993   \n",
              "2023-01-26  159.970001  161.419998  154.759995  160.270004  160.270004   \n",
              "2023-01-27  162.429993  180.679993  161.169998  177.899994  177.899994   \n",
              "2023-01-30  178.050003  179.770004  166.500000  166.660004  166.660004   \n",
              "2023-01-31  164.570007  174.300003  162.779999  173.220001  173.220001   \n",
              "2023-02-01  173.889999  183.809998  169.929993  181.410004  181.410004   \n",
              "2023-02-02  187.330002  196.750000  182.610001  188.270004  188.270004   \n",
              "2023-02-03  183.949997  198.990005  183.690002  189.979996  189.979996   \n",
              "\n",
              "                             AAPL                                      \\\n",
              "               Volume        Open        High         Low       Close   \n",
              "Date                                                                    \n",
              "2022-12-21  145417400  132.979996  136.809998  132.750000  135.449997   \n",
              "2022-12-22  210090300  134.350006  134.559998  130.300003  132.229996   \n",
              "2022-12-23  166989700  130.919998  132.419998  129.639999  131.860001   \n",
              "2022-12-27  208643400  131.380005  131.410004  128.720001  130.029999   \n",
              "2022-12-28  221070500  129.669998  131.029999  125.870003  126.040001   \n",
              "2022-12-29  221923300  127.989998  130.479996  127.730003  129.610001   \n",
              "2022-12-30  157304500  128.410004  129.949997  127.430000  129.929993   \n",
              "2023-01-03  231402800  130.279999  130.899994  124.169998  125.070000   \n",
              "2023-01-04  180389000  126.889999  128.660004  125.080002  126.360001   \n",
              "2023-01-05  157986300  127.129997  127.769997  124.760002  125.019997   \n",
              "2023-01-06  220575900  126.010002  130.289993  124.889999  129.619995   \n",
              "2023-01-09  190284000  130.470001  133.410004  129.889999  130.149994   \n",
              "2023-01-10  167642500  130.259995  131.259995  128.119995  130.729996   \n",
              "2023-01-11  183810800  131.250000  133.509995  130.460007  133.490005   \n",
              "2023-01-12  169400900  133.880005  134.259995  131.440002  133.410004   \n",
              "2023-01-13  180439300  132.029999  134.919998  131.660004  134.759995   \n",
              "2023-01-17  186477000  134.830002  137.289993  134.130005  135.940002   \n",
              "2023-01-18  195680300  136.820007  138.610001  135.029999  135.210007   \n",
              "2023-01-19  170291900  134.080002  136.250000  133.770004  135.270004   \n",
              "2023-01-20  138429900  135.279999  138.020004  134.220001  137.869995   \n",
              "2023-01-23  203119200  138.119995  143.320007  137.899994  141.110001   \n",
              "2023-01-24  158699100  140.309998  143.160004  140.300003  142.529999   \n",
              "2023-01-25  192734300  140.889999  142.429993  138.809998  141.860001   \n",
              "2023-01-26  234815100  143.169998  144.250000  141.899994  143.960007   \n",
              "2023-01-27  305632100  143.160004  147.229996  143.080002  145.929993   \n",
              "2023-01-30  230878800  144.960007  145.550003  142.850006  143.000000   \n",
              "2023-01-31  196813500  142.699997  144.339996  142.279999  144.289993   \n",
              "2023-02-01  213806300  143.970001  146.610001  141.320007  145.429993   \n",
              "2023-02-02  217448300  148.899994  151.179993  148.169998  150.820007   \n",
              "2023-02-03  229175706  148.029999  157.380005  147.830002  154.500000   \n",
              "\n",
              "                                   \n",
              "             Adj Close     Volume  \n",
              "Date                               \n",
              "2022-12-21  135.449997   85928000  \n",
              "2022-12-22  132.229996   77852100  \n",
              "2022-12-23  131.860001   63814900  \n",
              "2022-12-27  130.029999   69007800  \n",
              "2022-12-28  126.040001   85438400  \n",
              "2022-12-29  129.610001   75703700  \n",
              "2022-12-30  129.929993   76960600  \n",
              "2023-01-03  125.070000  112117500  \n",
              "2023-01-04  126.360001   89113600  \n",
              "2023-01-05  125.019997   80962700  \n",
              "2023-01-06  129.619995   87686600  \n",
              "2023-01-09  130.149994   70790800  \n",
              "2023-01-10  130.729996   63896200  \n",
              "2023-01-11  133.490005   69458900  \n",
              "2023-01-12  133.410004   71379600  \n",
              "2023-01-13  134.759995   57758000  \n",
              "2023-01-17  135.940002   63646600  \n",
              "2023-01-18  135.210007   69672800  \n",
              "2023-01-19  135.270004   58280400  \n",
              "2023-01-20  137.869995   79972200  \n",
              "2023-01-23  141.110001   81760300  \n",
              "2023-01-24  142.529999   66435100  \n",
              "2023-01-25  141.860001   65799300  \n",
              "2023-01-26  143.960007   54105100  \n",
              "2023-01-27  145.929993   70492800  \n",
              "2023-01-30  143.000000   64015300  \n",
              "2023-01-31  144.289993   65874500  \n",
              "2023-02-01  145.429993   77663600  \n",
              "2023-02-02  150.820007  118339000  \n",
              "2023-02-03  154.500000  149918017  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def getMultiStockDataDaily(symbols, day):\n",
        "    if len(symbols) > 1:\n",
        "        print(\"Length of symbols array is more than 1. STARTING ARRAYTOSTRING\")\n",
        "        symbols = arrayToString(symbols)\n",
        "    print(\"Getting stock data for stock $\"+symbols)\n",
        "    df = yf.download(symbols, start=day, period = \"1d\", group_by='ticker')\n",
        "    return df\n",
        "\n",
        "getMultiStockDataDaily(['AAPL', 'TSLA'], \"2022-12-21\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getMonthlyStockData(symbol, day = datetime.date.today() - datetime.timedelta(days = 1), interval = '1mo'):\n",
        "    print(\"Getting stock data for stock $\"+symbol)\n",
        "    df = yf.download(symbol, start=day, period = interval, group_by='ticker')\n",
        "    return df\n",
        "\n",
        "#getMonthlyStockData('AAPL', \"2022-11-21\", '1mo')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gathering FinViz Data (Today's News) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters \n",
        "n = 3 #the # of article headlines displayed per ticker\n",
        "tickers = ['AAPL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'news':        Date                                              Title  \\\n",
              " 0   04:38PM  Google workers protesting in California and Ne...   \n",
              " 1   04:31PM            Montana China spy balloon: What to know   \n",
              " 2   04:27PM  Stocks Close Lower, Yields Climb After Jobs Re...   \n",
              " 3   04:10PM  U.S. stocks finish lower, but Nasdaq Composite...   \n",
              " 4   04:10PM  Jobs report jolts Wall Street bulls as inflati...   \n",
              " ..      ...                                                ...   \n",
              " 85  06:37AM  Air raid alerts in Kyiv as EU leaders to hold ...   \n",
              " 86  06:32AM  Why Chinese Companies Are Investing Billions i...   \n",
              " 87  06:17AM  Samsung boss says he would not give a child un...   \n",
              " 88  06:08AM                     AMERICAS Runaway Tech arrested   \n",
              " 89  06:04AM  Germany Gives Green Light to Export Leopard 1 ...   \n",
              " \n",
              "                  Source                                               Link  \n",
              " 0       foxbusiness.com  https://foxbusiness.com/technology/google-work...  \n",
              " 1       foxbusiness.com  https://foxbusiness.com/politics/montana-china...  \n",
              " 2           www.wsj.com  https://www.wsj.com/livecoverage/stock-market-...  \n",
              " 3   www.marketwatch.com  http://www.marketwatch.com/news/story/us-stock...  \n",
              " 4       www.reuters.com  https://www.reuters.com/markets/us/jobs-report...  \n",
              " ..                  ...                                                ...  \n",
              " 85      edition.cnn.com  https://edition.cnn.com/webview/europe/live-ne...  \n",
              " 86      www.nytimes.com  https://www.nytimes.com/2023/02/03/business/ch...  \n",
              " 87        www.bbc.co.uk  https://www.bbc.co.uk/news/business-64504549?a...  \n",
              " 88      www.reuters.com  https://www.reuters.com/markets/us/global-mark...  \n",
              " 89    www.bloomberg.com  https://www.bloomberg.com/news/articles/2023-0...  \n",
              " \n",
              " [90 rows x 4 columns],\n",
              " 'blogs':        Date                                              Title  \\\n",
              " 0   04:20PM  More Recession Signs: Money Supply Growth Went...   \n",
              " 1   04:01PM  Nasdaq Soars To Best Start Since 1975 After Ja...   \n",
              " 2   03:40PM  Watch: Democrats Oppose Amendment To Recite Pl...   \n",
              " 3   03:20PM  These Were The Best And Worst Performing Asset...   \n",
              " 4   03:15PM  Suspected Chinese Spy Balloon Might Be Headed ...   \n",
              " ..      ...                                                ...   \n",
              " 85   Dec-18                           Your Mental Sharpe Ratio   \n",
              " 86   Dec-06   The Three Essential Sources of Your Trading Edge   \n",
              " 87   Nov-24                                Trading Consciously   \n",
              " 88   Nov-18  Relapse Prevention:  A Neglected Topic In Trad...   \n",
              " 89   May-21  New Issue Now Available: What Hedge Funds Boug...   \n",
              " \n",
              "                      Source                                               Link  \n",
              " 0         www.zerohedge.com  https://www.zerohedge.com/markets/more-recessi...  \n",
              " 1         www.zerohedge.com  https://www.zerohedge.com/markets/nasdaq-soars...  \n",
              " 2         www.zerohedge.com  https://www.zerohedge.com/political/watch-demo...  \n",
              " 3         www.zerohedge.com  https://www.zerohedge.com/markets/these-were-b...  \n",
              " 4         www.zerohedge.com  https://www.zerohedge.com/geopolitical/high-al...  \n",
              " ..                      ...                                                ...  \n",
              " 85  traderfeed.blogspot.com  http://traderfeed.blogspot.com/2022/12/your-me...  \n",
              " 86  traderfeed.blogspot.com  http://traderfeed.blogspot.com/2022/12/the-thr...  \n",
              " 87  traderfeed.blogspot.com  http://traderfeed.blogspot.com/2022/11/trading...  \n",
              " 88  traderfeed.blogspot.com  http://traderfeed.blogspot.com/2022/11/relapse...  \n",
              " 89      www.marketfolly.com  http://www.marketfolly.com/2022/05/new-issue-n...  \n",
              " \n",
              " [90 rows x 4 columns]}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from finvizfinance.news import News\n",
        "fnews = News()\n",
        "all_news = fnews.get_news()\n",
        "all_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current url is: https://finviz.com/quote.ashx?t=AAPL\n",
            "\n",
            "\n",
            "Recent News Headlines for AAPL: \n",
            "Tech Megacaps See Red as Earnings Disappoint ( Feb-03-23 04:27PM )\n",
            "US STOCKS-Wall Street ends down after stunning jobs growth raises Fed questions ( 04:17PM )\n",
            "Apple Sees Improved Outlook; Here's How It's Helping The Stock ( 04:03PM )\n"
          ]
        }
      ],
      "source": [
        "# Get Data\n",
        "finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
        "news_tables = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    url = finwiz_url + ticker\n",
        "    print(\"current url is: \" +url)\n",
        "    header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36.\"}\n",
        "    req = Request(url=url,headers=header) \n",
        "    resp = urlopen(req)    \n",
        "    html = BeautifulSoup(resp, features=\"lxml\")\n",
        "    news_table = html.find(id='news-table')\n",
        "    news_tables[ticker] = news_table\n",
        "\n",
        "try:\n",
        "    for ticker in tickers:\n",
        "        df = news_tables[ticker]\n",
        "        df_tr = df.findAll('tr')\n",
        "    \n",
        "        print ('\\n')\n",
        "        print ('Recent News Headlines for {}: '.format(ticker))\n",
        "        \n",
        "        for i, table_row in enumerate(df_tr):\n",
        "            a_text = table_row.a.text\n",
        "            td_text = table_row.td.text\n",
        "            td_text = td_text.strip()\n",
        "            print(a_text,'(',td_text,')')\n",
        "            if i == n-1:\n",
        "                break\n",
        "except KeyError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feb-03-23 04:27PMTech Megacaps See Red as Earnings Disappoint Investopedia\n",
            "04:17PMUS STOCKS-Wall Street ends down after stunning jobs growth raises Fed questions Reuters\n",
            "04:03PMApple Sees Improved Outlook; Here's How It's Helping The Stock Investor's Business Daily\n",
            "04:01PMStock market news live updates: Stocks slide after jobs report shocks, Big Tech results disappoint Yahoo Finance\n",
            "04:00PMUS STOCKS-Wall Street ends down after stunning jobs growth raises Fed questions Reuters\n",
            "\n",
            "03:37PM\n",
            "Loading…\n",
            "\n",
            "03:37PMApples earnings were a lot better than they look Yahoo Finance\n",
            "03:32PMBig Tech stocks: How Meta, Apple, Amazon are trading after earnings Yahoo Finance Video\n",
            "02:59PMWhy FAANG Stocks Made Big Moves Friday Motley Fool\n",
            "02:51PMWe've Dug Deeper Into 4 High Profile Portfolio Earnings Reports TheStreet.com\n",
            "02:45PMMaking Sense of Apple, Amazon and Big Tech Earnings Zacks\n",
            "02:31PM2023 Top Dow Jones Stocks To Buy And Watch In February: Apple Surges On Earnings Investor's Business Daily\n",
            "02:26PMUS STOCKS-Wall Street sinks after stunning jobs growth raises questions about Fed Reuters\n",
            "01:51PMBerkshire Hathaway Stock Is Losing to the S&P 500 This Year Barrons.com\n",
            "01:50PMMarket Update: AAPL, AMZN, BA, BYD, HSY, SKX, WEC, BMI, GM Argus Research\n",
            "01:43PMMarket Rally Powers Higher On Tame Fed, Meta Earnings; Apple, Google, Amazon In Focus: Weekly Review Investor's Business Daily\n",
            "\n",
            "01:34PM\n",
            "Loading…\n",
            "\n",
            "01:34PMApple Inc. (NASDAQ:AAPL) Q1 2023 Earnings Call Transcript Insider Monkey\n",
            "01:27PM10 Hot Tech Stocks To Buy Now Insider Monkey\n",
            "01:10PMStock Market Turns Lower After Strong Jobs Report; Apple Stock Reverses Higher Despite Sluggish Quarter Investor's Business Daily\n",
            "12:45PMWhy Apple Stock Was Up on Friday Motley Fool\n",
            "12:20PMApple Is Far Better Positioned Than Amazon or Alphabet: Here's the Trade TheStreet.com\n",
            "12:11PMStock Market Trims Losses After Strong Jobs Report; Apple Stock Surges 4%, Tesla Reverses Higher Investor's Business Daily\n",
            "12:04PMQualcomm Stock Rises. The Company Is Diversifying Away From Smartphones and Apple. Barrons.com\n",
            "11:17AMJanuary jobs report, Apple Q1 earnings, Nerdy stock surges on AI products: 3 things to watch markets Yahoo Finance Video\n",
            "11:10AMETFs in Focus on Apple's First Earnings Miss Since 2016 Zacks\n",
            "11:09AMApple stock rallies even as CEO Tim Cook spooks with one phrase Yahoo Finance\n",
            "11:09AMApple stock in focus as CEO Tim Cook spooks with one phrase Yahoo Finance\n",
            "10:45AMStock Market Recovers After Blowout Jobs Report; Apple, Google, AMZN, QCOM Earnings In Focus Investor's Business Daily\n",
            "10:38AMApple Blames Rotten Holiday Quarter on Supply Chain, Economy Bloomberg\n",
            "10:36AMGRAPHIC-Tech trillion club's wobble in four charts Reuters\n",
            "10:30AMStock Market Takes Heat After Blowout Jobs Report; Apple, Google, AMZN, QCOM Earnings In Focus Investor's Business Daily\n",
            "\n",
            "10:26AM\n",
            "Loading…\n",
            "\n",
            "10:26AMApple Sees Improved Outlook After iPhone Supply Issues Resolved Investor's Business Daily\n",
            "10:01AMApple is producing more. But now the worries are who will buy? MarketWatch\n",
            "10:00AMApple Stock Turns Higher As iPhone Outlook Offsets Rare Earnings Miss From China Disruptions TheStreet.com\n",
            "09:54AMDow Jones Slides On Strong January Jobs Report; Apple Reverses Higher; Alphabet, Amazon Drop On Earnings Investor's Business Daily\n",
            "09:39AMNasdaq Rally Gets a Reality Check as Megacaps Miss Bloomberg\n",
            "09:39AMHere's Our Take on the Latest Earnings and the Jobs Report TheStreet.com\n",
            "09:31AMApple cant supply enough  and thats a good problem, analyst says Yahoo Finance Video\n",
            "09:29AMTech trillion club's wobble in four charts Reuters\n",
            "09:22AMGRAPHIC-Tech trillion club's wobble in four charts Reuters\n",
            "09:15AMDow Jones Futures Slide On Strong January Jobs Report; Alphabet, Amazon, Apple Drop On Earnings Investor's Business Daily\n",
            "09:11AMAnalysis-From Meta to Microsoft, AI's big moment is here Reuters\n",
            "08:53AM2 Top Stocks to Buy in February Motley Fool\n",
            "08:35AMDow Jones Futures Drop On Strong January Jobs Report; Alphabet, Amazon, Apple Drop On Earnings Investor's Business Daily\n",
            "08:27AMApple Misses Earnings and Revenue Estimates. The Stock Is Falling. Barrons.com\n",
            "08:23AMDow Jones Futures Fall Ahead Of January Jobs Report; Alphabet, Amazon, Apple Drop On Earnings Investor's Business Daily\n",
            "08:19AM10 Best Dividend Stocks To Buy in February Motley Fool\n",
            "08:15AMDow Jones Futures Fall As Apple, Google, Amazon Skid, Jobs Report Looms; Market Rally Due For Pullback? Investor's Business Daily\n",
            "07:28AMApple, Alphabet and Amazon stock selloff would reduce their market caps by more than $150 billion combined MarketWatch\n",
            "07:14AMApple Stock Falls After Earnings Motley Fool\n",
            "07:10AMUPDATE 1-Tech earnings hit pause button on market rally Reuters\n",
            "07:09AMWhy Apples Demand Problem May Not Be That Bad Barrons.com\n",
            "06:32AMApple Is Not the Economy. Why the Feds More Bullish Than Tech. Barrons.com\n",
            "06:30AMTheyre baaaaack. Retail participation in the stock market just surpassed the GameStop days. MarketWatch\n",
            "06:24AMApple Stock Slumps After Rare Earnings Miss As China Disruptions Weaken iPhone Sales TheStreet.com\n",
            "06:18AMApple stock gets nailed as CEO Tim Cook spooks investors with one phrase Yahoo Finance\n",
            "06:00AMMORNING BID AMERICAS-Runaway Tech arrested Reuters\n",
            "05:54AMIndia's customs duty change to dial up local phone production-tax official Reuters\n",
            "05:42AMApple Sales Fall for First Time Since 2019. Just How Bad Is Its Demand Problem? Barrons.com\n",
            "05:21AMThis Is Warren Buffett's No. 1 Stock to Buy (and You Won't Find It in Berkshire Hathaway's Portfolio) Motley Fool\n",
            "05:20AMFutures fall as megacaps slide on downbeat earnings Reuters\n",
            "05:16AMUS STOCKS-Futures fall as megacaps slide on downbeat earnings Reuters\n",
            "05:13AMStocks Lower Ahead of Jobs Data, Apple, Amazon, Google, Nordstrom - Five Things To Know TheStreet.com\n",
            "05:06AMApple and Starbucks couldnt rise above Chinas zero-covid policy Quartz\n",
            "01:30AM$5tn of meh earnings Financial Times\n",
            "12:00AMMarkets dove-coloured glasses Financial Times\n",
            "Feb-02-23 11:05PMApple offers breadcrumbs for a forecast, but is that enough to reassure Wall Street? MarketWatch\n",
            "10:15PMApple Again Dominates Smartphone Profit, Taking Record 85% Share Bloomberg\n",
            "10:14PMDow Jones Futures: Apple, Google, Amazon Skid, Jobs Report Looms; Market Rally Due For Pullback? Investor's Business Daily\n",
            "09:06PMTech earnings hit pause button on market rally Reuters\n",
            "09:00PMApple (AAPL) Q1 2023 Earnings Call Transcript Motley Fool\n",
            "08:46PMAnalyst Report: Apple Inc. Morningstar Research\n",
            "08:22PMApple, Alphabet and Amazon Hurt as Economic Slump Crimps Demand Bloomberg\n",
            "08:07PMTim Cook, Apple CEO, weighs in after disappointing first-quarter revenue Fox Business\n",
            "08:04PMApple reports first decline in revenue in three-and-a-half years Financial Times\n",
            "07:17PMApple earnings: Twitter reacts to Q1 results, significant headwinds for tech industry Yahoo Finance Video\n",
            "06:51PMApple earnings: What to watch from the iPhone maker in the quarter ahead Yahoo Finance Video\n",
            "06:35PMApple Misses Estimates as iPhone Sales Decline Investopedia\n",
            "06:31PMApple earnings vibe check: Tech giant stresses bright spots after rare Q1 miss Yahoo Finance Video\n",
            "06:26PMApple posts disappointing first quarter results Fox Business\n",
            "06:12PMApple earnings show steepest sales decline in more than 6 years MarketWatch\n",
            "05:53PMStocks moving in after-hours: Amazon, Alphabet, Apple Yahoo Finance\n",
            "05:51PMMeta Platforms Shares Surge 23% on Cost Cuts, Stock Buyback The Wall Street Journal\n",
            "05:41PMApple Misses December-Quarter Targets As iPhone, Mac Sales Fall Investor's Business Daily\n",
            "05:03PMTech earnings: Breaking down the big takeaways from Apple, Amazon, Alphabet Yahoo Finance Video\n",
            "04:58PMApple's growth in services category is 'the positive takeaway' from earnings: Analyst Yahoo Finance Video\n",
            "04:56PMApple iPhone revenue misses estimates in Q1 earnings release Yahoo Finance Video\n",
            "04:45PMApple Sales Shrink as Covid Disruptions in China Slow Production The Wall Street Journal\n",
            "04:42PMApple Earnings Miss Forecast For First Time Since 2016 As iPhone Sales Hit By China Supply Chain Snarls TheStreet.com\n",
            "04:39PMApple earnings fall short on underwhelming sales of iPhones and Macs MarketWatch\n",
            "04:33PMApple misses Q1 earnings expectations as iPhone sales fall short Yahoo Finance\n",
            "04:33PMThese Stocks Are Moving the Most Today: Meta, Align, Coinbase, Harley, Okta, and More Barrons.com\n",
            "04:32PMApple's lower iPhone sales drive first profit miss since 2016 Reuters\n",
            "04:31PMUPDATE 2-Apple's lower iPhone sales drive first profit miss since 2016 Reuters\n",
            "04:30PMApple reports first quarter results Business Wire\n",
            "04:16PMWhat to look for in Apple, other Big Tech earnings Yahoo Finance Video\n",
            "04:15PMWhat to look for in Big Tech earnings Yahoo Finance Video\n",
            "03:35PMTech earnings preview: Analysts watch Apple iPhone numbers, Amazon sales growth Yahoo Finance Video\n",
            "03:27PMApple, Google called on by Democratic senator to ban TikTok in app stores Yahoo Finance Video\n",
            "03:21PMAhead of Apple earnings, did wireless companies send a troubling signal? MarketWatch\n",
            "02:45PMMetas earnings aren't a great sign for the ad market Yahoo Finance\n"
          ]
        }
      ],
      "source": [
        "# Iterate through the news\n",
        "parsed_news = []\n",
        "for file_name, news_table in news_tables.items():\n",
        "    for x in news_table.findAll('tr'):\n",
        "        print(x.get_text())\n",
        "        text = x.get_text() \n",
        "        date_scrape = x.td.text.split()\n",
        "\n",
        "        if len(date_scrape) == 1:\n",
        "            time = date_scrape[0]\n",
        "            \n",
        "        else:\n",
        "            date = date_scrape[0]\n",
        "            time = date_scrape[1]\n",
        "\n",
        "        ticker = file_name.split('_')[0]\n",
        "        \n",
        "        parsed_news.append([ticker, date, time, text ])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gathering Data From AlphaAdvantage for Historical News #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from decouple import config\n",
        "import requests\n",
        "import urllib.parse\n",
        "import json\n",
        "import datetime\n",
        "AAapikey = config('AAKey')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Function to convert user provided date to date required by AlphaAdvantage\n",
        "def toAADate(oldDate):\n",
        "    newDate = oldDate.strftime(\"%Y%m%dT0130\")\n",
        "    return str(newDate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting the breakdown\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n"
          ]
        }
      ],
      "source": [
        "#End Date is yesterday\n",
        "def breakdownofhistoric(ticker, endDate = datetime.date.today() - datetime.timedelta(days = 1), interval = 30, runs = 1):\n",
        "    print(\"Starting the breakdown\")\n",
        "    rollingWindow = 5\n",
        "    for i in range(interval):\n",
        "        dayDiff = i + 1\n",
        "        print(dayDiff)\n",
        "\n",
        "\n",
        "breakdownofhistoric('AAPL', interval = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "endDate is: 2023-02-02\n",
            "startDate is: 2023-01-03\n",
            "startDate is: 20230103T0130\n",
            "endDate is: 20230202T0130\n",
            "           Date                                           Headline Ticker\n",
            "0    2023-01-03              2 Stocks to Invest in Virtual Reality   AAPL\n",
            "1    2023-01-03  If You Invested $100 in Berkshire Hathaway in ...   AAPL\n",
            "2    2023-01-03     23 Top Dividend Stocks to Buy and Hold in 2023   AAPL\n",
            "3    2023-01-03  The Zacks Analyst Blog Highlights Apple, Johns...   AAPL\n",
            "4    2023-01-03  Should Schwab Fundamental U.S. Large Company I...   AAPL\n",
            "..          ...                                                ...    ...\n",
            "195  2023-01-09  Top Stories Monday, Jan. 09 - Apple  ( NASDAQ:...   AAPL\n",
            "196  2023-01-09  Apple  ( AAPL )  Expands Fitness+ With New Kic...   AAPL\n",
            "197  2023-01-09  This Day In Market History, Jan. 9: Apple Intr...   AAPL\n",
            "198  2023-01-09  Stock Market Gains As Earnings Season Begins; ...   AAPL\n",
            "199  2023-01-09  2 Stocks to Watch From the Challenging Compute...   AAPL\n",
            "\n",
            "[200 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Get data from AlphaAdvantage for one ticker for a particular day\n",
        "def getHistoricNewsData(ticker, endDate = datetime.date.today() - datetime.timedelta(days = 1) , interval = '1mo'):\n",
        "    url = 'https://www.alphavantage.co/query?'\n",
        "    print(\"endDate is: \" +str(endDate))\n",
        "    if interval == '1mo':\n",
        "        days_to_add = 30\n",
        "    else:\n",
        "        days_to_add = 60\n",
        "    delta_days = datetime.timedelta(days = days_to_add)\n",
        "    startDate = endDate - delta_days\n",
        "    print(\"startDate is: \" +str(startDate))\n",
        "    if startDate and endDate:\n",
        "        startDate = toAADate(startDate)\n",
        "        endDate = toAADate(endDate)\n",
        "        print(\"startDate is: \" +str(startDate))\n",
        "        print(\"endDate is: \" +str(endDate))\n",
        "        Myparams = {'function': 'NEWS_SENTIMENT', 'tickers': ticker, 'time_from': startDate, 'time_to': endDate, 'sort': 'EARLIEST','limit': 200, 'apikey': AAapikey}\n",
        "    else:\n",
        "        print(\"NEED DATES\")\n",
        "        #Myparams = {'function': 'NEWS_SENTIMENT', 'tickers': ticker, 'sort': 'LATEST','limit': 100, 'apikey': AAapikey}\n",
        "    r = requests.get(url, params = Myparams)\n",
        "    data = r.json()\n",
        "    #return data\n",
        "    historic_news = pd.DataFrame(columns=['Date', 'Headline', 'Ticker'])\n",
        "    for i in data.get(\"feed\"):\n",
        "        test_date = i.get(\"time_published\")\n",
        "        test_date = test_date[:8]\n",
        "        newDate = datetime.datetime.strptime(test_date, '%Y%m%d').date()\n",
        "        row = [newDate, i.get(\"title\"), ticker]\n",
        "        new_df = pd.DataFrame([row],columns=['Date', 'Headline', 'Ticker'])\n",
        "        historic_news = pd.concat([historic_news, new_df], axis=0, ignore_index=True)\n",
        "    return historic_news\n",
        "\n",
        "#historic_news = getHistoricNewsData('AAPL', '2022-10-10', '2mo')\n",
        "historic_news = getHistoricNewsData('AAPL', interval = '1mo')\n",
        "print(historic_news)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment Analysis of News data #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SentimentAnalysisNewsData(parsedNews, printOut = False):\n",
        "    #Downloading Vader Lexicon for Sentiment Analysis\n",
        "    nltk.download('vader_lexicon')\n",
        "    # Initializing Sentiment Analysis\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    #Declaring Column Names\n",
        "    columns = ['Ticker', 'Date', 'Time', 'Headline']\n",
        "    #Creating dataframe from news\n",
        "    news = pd.DataFrame(parsedNews, columns=columns)\n",
        "    print(news)\n",
        "    #Getting scores for headlines\n",
        "    scores = news['Headline'].apply(analyzer.polarity_scores).tolist()\n",
        "\n",
        "    #Creating Dataframe of Scores\n",
        "    df_scores = pd.DataFrame(scores)\n",
        "    #Joining scores to news dataframe\n",
        "    news = news.join(df_scores, rsuffix='_right')\n",
        "    #Converting Date column to pd datetime date\n",
        "    news['Date'] = pd.to_datetime(news.Date).dt.date\n",
        "\n",
        "    #List of unique tickers\n",
        "    unique_ticker = news['Ticker'].unique().tolist()\n",
        "    #Creating dict for news based on ticker\n",
        "    news_dict = {name: news.loc[news['Ticker'] == name] for name in unique_ticker}\n",
        "    #Initializing List of values\n",
        "    # og values = []\n",
        "    values = pd.DataFrame(columns = ['Ticker', 'Date', 'Compound'])\n",
        "    for ticker in tickers: \n",
        "        dataframe = news_dict[ticker]\n",
        "        dataframe = dataframe.set_index('Ticker')\n",
        "        #Dropping headlines column since we only need scores now\n",
        "        dataframe = dataframe.drop(columns = ['Headline'])\n",
        "        #if printOut:\n",
        "            #print ('\\n')\n",
        "            #print (dataframe.head())\n",
        "        \n",
        "        #mean = round(dataframe['compound'].mean(), 2)\n",
        "        #Finding compound number for news of every day\n",
        "        testdf = pd.DataFrame(columns = ['Date', 'Mean Sentiment'])\n",
        "        testdf = round(dataframe.groupby('Date')['compound'].mean(), 2)\n",
        "        print(testdf.shape)\n",
        "        #Adding values to values list\n",
        "        #og values.append(mean)\n",
        "    \n",
        "   \n",
        "    #print(round(dataframe.groupby('Date')['compound'].mean(), 2))\n",
        "    #print(\"VALUES------------\")\n",
        "    #print(values)\n",
        "        \n",
        "    #Combining tickers and values into new dataframe\n",
        "    #df = pd.DataFrame(list(zip(tickers, values)), columns =['Ticker', 'Mean Sentiment']) \n",
        "    #df = df.set_index('Ticker')\n",
        "    #df = df.sort_values('date', ascending=False)\n",
        "    if printOut:\n",
        "        print(\"-----------DF\")\n",
        "        #print(df.head())\n",
        "        #print(df.shape)\n",
        "    #Returning the dataframe\n",
        "    return df\n",
        "    #if printOut:\n",
        "        #print ('\\n')\n",
        "        #display (df)\n",
        "    #return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Ticker        Date  Time  \\\n",
            "0     AAPL  2023-01-03   NaN   \n",
            "1     AAPL  2023-01-03   NaN   \n",
            "2     AAPL  2023-01-03   NaN   \n",
            "3     AAPL  2023-01-03   NaN   \n",
            "4     AAPL  2023-01-03   NaN   \n",
            "..     ...         ...   ...   \n",
            "195   AAPL  2023-01-09   NaN   \n",
            "196   AAPL  2023-01-09   NaN   \n",
            "197   AAPL  2023-01-09   NaN   \n",
            "198   AAPL  2023-01-09   NaN   \n",
            "199   AAPL  2023-01-09   NaN   \n",
            "\n",
            "                                              Headline  \n",
            "0                2 Stocks to Invest in Virtual Reality  \n",
            "1    If You Invested $100 in Berkshire Hathaway in ...  \n",
            "2       23 Top Dividend Stocks to Buy and Hold in 2023  \n",
            "3    The Zacks Analyst Blog Highlights Apple, Johns...  \n",
            "4    Should Schwab Fundamental U.S. Large Company I...  \n",
            "..                                                 ...  \n",
            "195  Top Stories Monday, Jan. 09 - Apple  ( NASDAQ:...  \n",
            "196  Apple  ( AAPL )  Expands Fitness+ With New Kic...  \n",
            "197  This Day In Market History, Jan. 9: Apple Intr...  \n",
            "198  Stock Market Gains As Earnings Season Begins; ...  \n",
            "199  2 Stocks to Watch From the Challenging Compute...  \n",
            "\n",
            "[200 rows x 4 columns]\n",
            "(7,)\n",
            "-----------DF\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\Ishaan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#print(\"HISTORIC SENTIMENT\")\n",
        "HistoricSentiment = SentimentAnalysisNewsData(historic_news, True)\n",
        "#print(\"\\n\")\n",
        "#print(\"TODAYS SENTIMENT\")\n",
        "#TodaysSentiment = SentimentAnalysisNewsData(parsed_news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating Dataset #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating dataset for $AAPL\n",
            "Getting stock data for stock $AAPL\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "                  Open        High         Low       Close   Adj Close  \\\n",
            "Date                                                                     \n",
            "2023-02-02  148.899994  151.179993  148.169998  150.820007  150.820007   \n",
            "2023-02-03  148.029999  157.380005  147.830002  154.500000  154.500000   \n",
            "\n",
            "               Volume  \n",
            "Date                   \n",
            "2023-02-02  118339000  \n",
            "2023-02-03  149918017  \n",
            "endDate is: 2023-02-02\n",
            "startDate is: 2022-12-04\n",
            "startDate is: 20221204T0130\n",
            "endDate is: 20230202T0130\n",
            "           Date                                           Headline Ticker\n",
            "0    2022-12-04  1 ETF That Could Turn $200 Per Month Into Near...   AAPL\n",
            "1    2022-12-04    3 Stocks Billionaires Have Bought Ahead of 2023   AAPL\n",
            "2    2022-12-04  Canada's Largest Pension Sells Apple Stock, Sl...   AAPL\n",
            "3    2022-12-04      1 Green Flag and 1 Red Flag for TSMC's Future   AAPL\n",
            "4    2022-12-04  Is It Time to Buy the Dow Jones' 2 Worst-Perfo...   AAPL\n",
            "..          ...                                                ...    ...\n",
            "195  2022-12-09  Penny Stocks Chart School: Descending Wedge De...   AAPL\n",
            "196  2022-12-09  Real Calamity Far Outpaces Virtual Reality At ...   AAPL\n",
            "197  2022-12-09  3 No-Brainer Stocks to Buy Amid the Tech Sell-Off   AAPL\n",
            "198  2022-12-09  'Who Should We Acquire Next?' - Unphased by th...   AAPL\n",
            "199  2022-12-09  Never Lose Your Things Again - The Startup Pou...   AAPL\n",
            "\n",
            "[200 rows x 3 columns]\n",
            "    Ticker        Date  Time  \\\n",
            "0     AAPL  2022-12-04   NaN   \n",
            "1     AAPL  2022-12-04   NaN   \n",
            "2     AAPL  2022-12-04   NaN   \n",
            "3     AAPL  2022-12-04   NaN   \n",
            "4     AAPL  2022-12-04   NaN   \n",
            "..     ...         ...   ...   \n",
            "195   AAPL  2022-12-09   NaN   \n",
            "196   AAPL  2022-12-09   NaN   \n",
            "197   AAPL  2022-12-09   NaN   \n",
            "198   AAPL  2022-12-09   NaN   \n",
            "199   AAPL  2022-12-09   NaN   \n",
            "\n",
            "                                              Headline  \n",
            "0    1 ETF That Could Turn $200 Per Month Into Near...  \n",
            "1      3 Stocks Billionaires Have Bought Ahead of 2023  \n",
            "2    Canada's Largest Pension Sells Apple Stock, Sl...  \n",
            "3        1 Green Flag and 1 Red Flag for TSMC's Future  \n",
            "4    Is It Time to Buy the Dow Jones' 2 Worst-Perfo...  \n",
            "..                                                 ...  \n",
            "195  Penny Stocks Chart School: Descending Wedge De...  \n",
            "196  Real Calamity Far Outpaces Virtual Reality At ...  \n",
            "197  3 No-Brainer Stocks to Buy Amid the Tech Sell-Off  \n",
            "198  'Who Should We Acquire Next?' - Unphased by th...  \n",
            "199  Never Lose Your Things Again - The Startup Pou...  \n",
            "\n",
            "[200 rows x 4 columns]\n",
            "(6,)\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\Ishaan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "def createDataset(date_from, int):\n",
        "    for i in tickers:\n",
        "        print(\"Creating dataset for $\" +i)\n",
        "        #Get historic stock data\n",
        "        historic_stock = getMonthlyStockData(i, interval = int)\n",
        "        print(historic_stock.head())\n",
        "        #Get historic news data\n",
        "        historic_news = getHistoricNewsData(i, interval = int)\n",
        "        print(historic_news)\n",
        "        #Use news to get sentiment\n",
        "        HistoricSentiment = SentimentAnalysisNewsData(historic_news)\n",
        "        print(HistoricSentiment.shape)\n",
        "        #Merge as training set\n",
        "        #Get today's stock data\n",
        "        #Get today's news data\n",
        "        #Use news to get sentiment\n",
        "\n",
        "\n",
        "createDataset('2022-10-10', '2mo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO2ChGGwJkijmmb0vDcCR40",
      "include_colab_link": true,
      "name": "Sentiment Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
